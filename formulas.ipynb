{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENTROPIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Entropias: \n",
    "H(X) = - sumatoria desde 0 a N de p(x)*log(p(x)) = sumatoria desde 0 a N de p(x)*log(1/p(x))\n",
    "H(X, Y) = - sumatorias desde 0 a N de p(x,y)*log(p(x,y)) = sumatorias desde 0 a N de p(x,y)*log(1/p(x,y)) --> es una doble sumatoria (de todos los x y de todos los y)\n",
    "H(X|Y) = - sumatoria (solo de los x) desde 0 a N de p(x)*H(Y|X=x) = - sumatoria (de todos los x) desde 0 a N de p(x) * sumatoria (de todos los y) desde 0 a N de p(y|x)*log(p(y|x)) = - doble sumatoria de p(x, y)*log(p(y|x))\n",
    "\n",
    "Propiedades:\n",
    "H(X) >= 0\n",
    "H(X) = 0 si y solo si X es una variable aleatoria deterministica\n",
    "H(X, Y) = H(X) + H(Y|X) = H(Y) + H(X|Y) --> se demuestra por definicion separando p(x,y) en p(x)*p(y|x) y usando propiedades del logaritmo\n",
    "H(X, Y) = H(X) + H(Y) si X e Y son independientes\n",
    "H(X, Y|Z) = H(X|Z) + H(Y|X, Z)\n",
    "H(X) - H(X|Y) = H(Y) - H(Y|X)\n",
    "H(Y|X) != H(X|Y)\n",
    "\n",
    "Entropa relativa: medida de la distancia entre dos distribuciones (la divergencia entre p y q es lo que le falta a p para ser q)\n",
    "** Lo que acá se llama distancia no es una distancia real ya que no satisface la desigualdad triangular ni tampoco es simétrica. \n",
    "D(p||q) = sumatoria (de todas las x)desde 0 a N de p(x)*log(p(x)/q(x))\n",
    "\n",
    "Informacion mutua: medida de la información que una variable aleatoria contiene sobre otra variable aleatoria \n",
    "(Cuánto le falta a la conjunta para ser una variable independiente)\n",
    "I(X, Y) = sumatorias de 0 a N de p(x, y)*log((p(x, y))/p(x)*p(y)) = D(p(x, y)||p(x)*p(y)) \n",
    "\n",
    "Propiedades:\n",
    "I(X, Y) = H(Y) - H(Y|X) --> lo demuestro escribiendo p(x, y) como p(x|y)*p(y) y se me cancela con el p(y) de la division (todo eso adentro del log que aparece por partir de la definicion de informacion mutua)\n",
    "I(X, Y) = H(X) - H(X|Y)\n",
    "I(X, Y) = H(X) + H(Y) - H(X, Y) --> lo demuestro separando el log de la division por la resta de los logaritmos\n",
    "I(X, Y) = I(Y, X)\n",
    "I(X, X) = H(X)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Desigualdad de Jensen:\n",
    "p1*f(x1) + p2*f(x2) + ... + pn*f(xn) >= f(p1*x1 + p2*x2 + ... + pn*xn) --> con esto demuestro que D(p||q) >= 0: parto de la definicion agregandole un menos a ambos lados del igual\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
